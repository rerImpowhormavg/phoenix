---
description: Troubleshooting an LLM Summarization Task
---

# Generative LLMs

Imagine you're responsible for your media company's summarization model that condenses daily news into concise summaries. Your model's performance has recently declined, leading to negative feedback from readers around the globe.

This tutorial shows how Phoenix help you find the root-cause of LLM performance issues by analyzing prompt-response pairs.

In this tutorial, you will:

* Download curated LLM data for this walkthrough
* Compute embeddings for each prompt (article) and response (summary)
* Calculate ROUGE-L scores to evaluate the quality of your LLM-generated summaries against human-written reference summaries
* Launch Phoenix
* Find articles that your LLM is struggling to summarize

Open the tutorial in Colab or GitHub to get started.
