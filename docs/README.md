---
description: Evaluate, troubleshoot, and fine-tune your LLM, CV, and NLP models.
---

# Arize Phoenix

Phoenix is an open-source observability library and platform designed for experimentation, evaluation, and troubleshooting.

The toolset is designed to ingest [inference data](inferences/inferences.md) for [LLMs](user-guide.md), CV, NLP, and tabular datasets as well as [LLM traces](tracing/quickstart.md). It allows AI Engineers and Data Scientists to quickly visualize their data, evaluate performance, track down issues & insights, and easily export to improve.&#x20;

{% embed url="https://www.loom.com/share/a96e244c4ff8473d9350b02ccbd203b4" %}
Overview of Phoenix Tracing
{% endembed %}

## Install Phoenix

In your Jupyter or Colab environment, run the following command to install.

{% tabs %}
{% tab title="Using pip" %}
```sh
pip install arize-phoenix
```
{% endtab %}

{% tab title="Using conda" %}
```sh
conda install -c conda-forge arize-phoenix
```
{% endtab %}
{% endtabs %}

Note that the above only installs dependencies that are necessary to run the application. Phoenix also has an experimental sub-module where you can find [LLM Evals](evaluation/overview.md).

```sh
pip install arize-phoenix[experimental]
```

## Quickstarts

Running Phoenix for the first time? Select a quickstart below.&#x20;

<table data-card-size="large" data-view="cards"><thead><tr><th align="center"></th><th data-hidden data-card-target data-type="content-ref"></th><th data-hidden data-card-cover data-type="files"></th></tr></thead><tbody><tr><td align="center"><strong>Tracing</strong></td><td><a href="tracing/quickstart.md">quickstart.md</a></td><td><a href=".gitbook/assets/Screenshot 2023-09-27 at 1.51.45 PM.png">Screenshot 2023-09-27 at 1.51.45 PM.png</a></td></tr><tr><td align="center"><strong>Inferences</strong></td><td><a href="inferences/quickstart.md">quickstart.md</a></td><td><a href=".gitbook/assets/Screenshot 2023-09-27 at 1.53.06 PM.png">Screenshot 2023-09-27 at 1.53.06 PM.png</a></td></tr></tbody></table>

Don't know which one to choose? Phoenix has two main data ingestion methods:

1. [LLM Traces:](tracing/quickstart.md) Phoenix is used on top of trace data generated by LlamaIndex and LangChain. The general use case is to troubleshoot LLM applications with agentic workflows.&#x20;
2. [Inferences](inferences/quickstart.md): Phoenix is used to troubleshoot models whose datasets can be expressed as DataFrames in Python such as LLM applications built in Python workflows, CV, NLP, and tabular models.

###

## Resources

### [Tutorials](notebooks.md)

Check out a comprehensive list of example notebooks for LLM Traces, Evals, RAG Analysis, and more. &#x20;

### [Use Cases](broken-reference)

Learn about best practices, and how to get started with use case examples such as Q\&A with Retrieval, Summarization, and Chatbots.&#x20;

### [Community](https://join.slack.com/t/arize-ai/shared\_invite/zt-1ppbtg5dd-1CYmQO4dWF4zvXFiONTjMg)

Join the Phoenix Slack community to ask questions, share findings, provide feedback, and connect with other developers.&#x20;

